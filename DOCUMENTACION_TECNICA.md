# Documentaci√≥n T√©cnica: Evaluaci√≥n de Kolmogorov-Arnold Networks en Clasificaci√≥n de C√°ncer de Mama

## √çndice
1. [Introducci√≥n](#introducci√≥n)
2. [Estructura del Proyecto](#estructura-del-proyecto)
3. [Configuraci√≥n del Entorno](#configuraci√≥n-del-entorno)
4. [Carga y Preprocesamiento de Datos](#carga-y-preprocesamiento-de-datos)
5. [Arquitecturas de Modelos](#arquitecturas-de-modelos)
6. [Proceso de Entrenamiento](#proceso-de-entrenamiento)
7. [Evaluaci√≥n y M√©tricas](#evaluaci√≥n-y-m√©tricas)
8. [An√°lisis de Resultados](#an√°lisis-de-resultados)
9. [Conclusiones](#conclusiones)

---

## Resumen Ejecutivo

### Resultados Clave
Este estudio evalu√≥ cinco arquitecturas de redes neuronales para clasificaci√≥n de c√°ncer de mama:

| Modelo | Sensitivity | Specificity | AUC-ROC | MCC | Recomendaci√≥n |
|--------|------------|-------------|---------|-----|---------------|
| **Baseline MLP** | 0.9524 | **1.0000** | 0.9954 | **0.9626** | **Producci√≥n** |
| **Chebyshev-KAN V4** | **1.0000** | 0.9444 | **0.9980** | 0.9286 | **Screening** |
| **Fast-KAN V3** | 0.9762 | 0.9722 | 0.9874 | 0.9439 | Balance |
| **Wavelet-KAN V3** | 0.8571 | 0.9861 | 0.9792 | 0.8688 | Investigaci√≥n |
| **Fourier-KAN V4** | 0.8571 | 0.9167 | 0.9649 | 0.7738 | No recomendado |

### Hallazgos Principales

1. **KAN vs MLP**: Las redes KAN **no superan universalmente** a MLP, pero ofrecen trade-offs valiosos
2. **Mejor para Screening**: **Chebyshev-KAN V4** (Sensitivity perfecta 1.0000, 0 falsos negativos)
3. **Mejor Balance General**: **Baseline MLP** (MCC 0.9626, Specificity perfecta 1.0000)
4. **Interpretabilidad**: KAN permite an√°lisis de coeficientes funcionales (ventaja sobre MLP)
5. **Estabilizaci√≥n Cr√≠tica**: RBF requiere centros fijos; Dropout debe ajustarse por arquitectura

### Recomendaci√≥n Pr√°ctica

**Para Screening Masivo en Hospitales**:
- Usar **Chebyshev-KAN V4** (ning√∫n caso de c√°ncer perdido)
- Aceptar 4 falsos positivos (biopsias innecesarias) vs salvar vidas

**Para Producci√≥n Robusta**:
- Usar **Baseline MLP** (arquitectura madura, menor complejidad, mejor MCC)

**Para Investigaci√≥n Cl√≠nica**:
- Usar **Chebyshev-KAN V4** (coeficientes interpretables para an√°lisis biomarcadores)

---

## Introducci√≥n

Este proyecto implementa y eval√∫a diferentes variantes de Kolmogorov-Arnold Networks (KAN) para la clasificaci√≥n binaria de tumores mamarios utilizando el dataset Wisconsin Diagnostic Breast Cancer (WDBC).

### Objetivo Principal
Comparar el desempe√±o de cuatro variantes de redes KAN (Chebyshev V4, Wavelet V3, Fast-RBF V3, Fourier V4) contra una arquitectura MLP tradicional en un problema de clasificaci√≥n m√©dica real.

### Contexto Cl√≠nico
En diagn√≥stico de c√°ncer, los errores de clasificaci√≥n tienen implicaciones diferentes:
- **Falsos Negativos (FN)**: **CR√çTICO** - Pacientes con c√°ncer no detectados (no reciben tratamiento)
- **Falsos Positivos (FP)**: Menos cr√≠tico - Biopsias adicionales innecesarias (inconvenientes pero no letales)
- **Objetivo Primario**: Maximizar Sensibilidad (idealmente 100%) para no perder ning√∫n caso de c√°ncer
- **Objetivo Secundario**: Mantener Especificidad alta (>90%) para minimizar biopsias innecesarias

### Contexto T√©cnico
**Kolmogorov-Arnold Networks (KAN)** son arquitecturas que reemplazan activaciones fijas por funciones base aprendibles:
- **Teorema de Kolmogorov-Arnold**: Toda funci√≥n continua multivariable puede representarse como suma de composiciones de funciones univariables
- **KAN**: Implementa este teorema usando bases funcionales (polinomios, wavelets, Fourier, RBF) en lugar de activaciones fijas (ReLU, GELU)
- **Ventaja Te√≥rica**: Mayor expresividad y potencial interpretabilidad mediante an√°lisis de coeficientes

---

## Estructura del Proyecto

```
taller-3/
‚îÇ
‚îú‚îÄ‚îÄ KAN_Wisconsin_BreastCancer.ipynb    # Notebook principal con implementaci√≥n completa
‚îú‚îÄ‚îÄ DOCUMENTACION_TECNICA.md            # Este archivo - Documentaci√≥n completa del proyecto
‚îú‚îÄ‚îÄ ANALISIS_MEJORAS_KAN.md             # An√°lisis detallado de optimizaciones V3/V4
‚îú‚îÄ‚îÄ GUIA_RAPIDA_MEJORAS_V4.md           # Gu√≠a r√°pida visual con diagramas
‚îú‚îÄ‚îÄ GUIA_RAPIDA_ESTABILIDAD_V3.md       # Mejoras de estabilidad (Wavelet, Fast-KAN)
‚îú‚îÄ‚îÄ MEJORAS_ESTABILIDAD_V3.md           # Detalle de mejoras de estabilidad
‚îú‚îÄ‚îÄ README.md                            # Documentaci√≥n general del repositorio
‚îú‚îÄ‚îÄ pyproject.toml                       # Configuraci√≥n de dependencias (uv)
‚îî‚îÄ‚îÄ check_environment.py                 # Script de verificaci√≥n de entorno
```

### Navegaci√≥n R√°pida por la Documentaci√≥n

- **üöÄ Inicio R√°pido**: Leer `README.md`
- **üìö Entender el C√≥digo**: Leer `DOCUMENTACION_TECNICA.md` (este archivo)
- **üî¨ Analizar Mejoras**: Leer `ANALISIS_MEJORAS_KAN.md`
- **üíª Implementar**: Ejecutar `KAN_Wisconsin_BreastCancer.ipynb`
- **‚úÖ Verificar Entorno**: Ejecutar `python check_environment.py`

### Flujo del Notebook Principal

El notebook `KAN_Wisconsin_BreastCancer.ipynb` sigue esta estructura:

```
1. INTRODUCCI√ìN Y CONTEXTO
   ‚îî‚îÄ Mejoras implementadas (V3/V4)

2. FASE 1: Carga y Preprocesamiento
   ‚îú‚îÄ Imports y configuraci√≥n (semilla, device)
   ‚îú‚îÄ Carga de WDBC dataset (569 muestras, 30 features)
   ‚îî‚îÄ Divisi√≥n estratificada (60-20-20) + estandarizaci√≥n

3. FASE 2: Implementaci√≥n de Arquitecturas
   ‚îú‚îÄ Baseline MLP (referencia tradicional)
   ‚îú‚îÄ Chebyshev-KAN V4 (polinomios ortogonales)
   ‚îú‚îÄ Wavelet-KAN V3 (Mexican Hat wavelets)
   ‚îú‚îÄ Fast-KAN V3 (RBF Gaussianas)
   ‚îî‚îÄ Fourier-KAN V4 (series de Fourier)

4. FASE 3: Framework de Entrenamiento
   ‚îú‚îÄ calculate_clinical_metrics() ‚Üí M√©tricas m√©dicas
   ‚îú‚îÄ train_epoch() ‚Üí √âpoca de entrenamiento
   ‚îú‚îÄ evaluate_model() ‚Üí Evaluaci√≥n en val/test
   ‚îî‚îÄ train_and_evaluate() ‚Üí Loop completo con early stopping

5. FASE 4: Entrenamiento (√öNICO)
   ‚îî‚îÄ Entrena los 5 modelos con versiones optimizadas finales
      (~10-15 minutos total)

6. FASE 5: An√°lisis y Visualizaciones
   ‚îú‚îÄ Comparaci√≥n de m√©tricas (tabla)
   ‚îú‚îÄ Ranking por criterio cl√≠nico
   ‚îú‚îÄ Gr√°ficos de barras comparativos
   ‚îú‚îÄ Curvas ROC superpuestas
   ‚îú‚îÄ Matrices de confusi√≥n
   ‚îú‚îÄ Curvas de entrenamiento (loss, sensitivity, specificity)
   ‚îî‚îÄ Gr√°ficos radar de m√©tricas

7. FASE 6: Conclusiones
   ‚îî‚îÄ An√°lisis profundo de por qu√© cada arquitectura funciona diferente
```

### Estado del Proyecto

**Versi√≥n Actual**: 4.0 (Octubre 2025)

**Modelos Implementados**:
- ‚úÖ Baseline MLP (referencia tradicional)
- ‚úÖ Chebyshev-KAN V4 (optimizado para screening, sensitivity perfecta)
- ‚úÖ Wavelet-KAN V3 (estabilizado anti-overfitting)
- ‚úÖ Fast-KAN V3 (centros RBF fijos para estabilidad)
- ‚úÖ Fourier-KAN V4 (arquitectura profunda, necesita optimizaci√≥n adicional)

**Estado de Entrenamiento**: ‚úÖ Completo  
**Resultados Validados**: ‚úÖ S√≠ (conjunto de prueba con 114 muestras)  
**Documentaci√≥n**: ‚úÖ Completa (este documento + ANALISIS_MEJORAS_KAN.md)

---

## Configuraci√≥n del Entorno

### Librer√≠as Requeridas

#### Deep Learning (PyTorch)
```python
import torch                 # Framework principal para redes neuronales
import torch.nn as nn        # M√≥dulos de redes neuronales (capas, activaciones)
import torch.optim as optim  # Optimizadores (Adam, AdamW, SGD)
from torch.utils.data import DataLoader, TensorDataset
```

**Prop√≥sito**: PyTorch proporciona las herramientas fundamentales para construir, entrenar y evaluar redes neuronales profundas con soporte para GPU.

#### Machine Learning (Scikit-learn)
```python
from sklearn.datasets import load_breast_cancer      # Dataset WDBC
from sklearn.model_selection import train_test_split # Divisi√≥n de datos
from sklearn.preprocessing import StandardScaler     # Estandarizaci√≥n
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_curve, auc, roc_auc_score, matthews_corrcoef
)
```

**Prop√≥sito**: Scikit-learn proporciona utilidades para preprocesamiento y m√©tricas de evaluaci√≥n est√°ndar en machine learning.

#### An√°lisis y Visualizaci√≥n
```python
import numpy as np           # Operaciones num√©ricas y √°lgebra lineal
import pandas as pd          # Manipulaci√≥n de datos tabulares
import matplotlib.pyplot as plt  # Visualizaci√≥n b√°sica
import seaborn as sns        # Visualizaci√≥n estad√≠stica avanzada
```

**Prop√≥sito**: Estas librer√≠as permiten an√°lisis exploratorio, manipulaci√≥n de datos y generaci√≥n de visualizaciones profesionales.

### Configuraci√≥n de Reproducibilidad

```python
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)  # Semilla para PyTorch
np.random.seed(RANDOM_SEED)     # Semilla para NumPy
```

**Prop√≥sito**: Fijar semillas aleatorias asegura que los resultados sean reproducibles en m√∫ltiples ejecuciones. Esto es cr√≠tico para validaci√≥n cient√≠fica y debugging.

### Detecci√≥n de Dispositivo

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

**Prop√≥sito**: Detecta autom√°ticamente si hay GPU disponible. Las GPU aceleran significativamente el entrenamiento de redes neuronales (10-100x m√°s r√°pido que CPU).

---

## Carga y Preprocesamiento de Datos

### Dataset: Wisconsin Diagnostic Breast Cancer (WDBC)

#### Caracter√≠sticas del Dataset
- **Muestras**: 569 biopsias de tumores mamarios
- **Caracter√≠sticas**: 30 caracter√≠sticas morfol√≥gicas continuas
- **Clases**: 2 (Maligno: 212 casos, Benigno: 357 casos)
- **Desbalanceo**: 37.3% malignos, 62.7% benignos

#### Origen de las Caracter√≠sticas
Las caracter√≠sticas se calculan de im√°genes digitalizadas de aspiraci√≥n con aguja fina (FNA) de masa mamaria. Para cada n√∫cleo celular se extraen:

**Caracter√≠sticas Base** (10):
1. Radio (distancia media del centro al per√≠metro)
2. Textura (desviaci√≥n est√°ndar de valores en escala de grises)
3. Per√≠metro
4. √Årea
5. Suavidad (variaci√≥n local en longitudes de radio)
6. Compacidad (per√≠metro¬≤ / √°rea - 1.0)
7. Concavidad (severidad de porciones c√≥ncavas del contorno)
8. Puntos c√≥ncavos (n√∫mero de porciones c√≥ncavas del contorno)
9. Simetr√≠a
10. Dimensi√≥n fractal ("aproximaci√≥n de l√≠nea costera" - 1)

**Medidas** (3 por caracter√≠stica = 30 total):
- Media
- Error est√°ndar
- "Peor" o mayor (promedio de los tres valores m√°s grandes)

### C√≥digo de Carga

```python
data = load_breast_cancer()
X = data.data       # Matriz (569, 30)
y = data.target     # Vector (569,)

# Inversi√≥n de etiquetas: 0=Benigno, 1=Maligno
y = 1 - y
```

**Justificaci√≥n de la inversi√≥n**: Por convenci√≥n en medicina, la clase positiva (1) representa la condici√≥n de inter√©s (enfermedad). Esto facilita la interpretaci√≥n de m√©tricas como Sensitivity (recall de clase positiva).

### Divisi√≥n Estratificada

#### Estrategia de Divisi√≥n
```
Dataset (569 muestras)
    ‚îÇ
    ‚îú‚îÄ Entrenamiento (60%): 341 muestras
    ‚îÇ
    ‚îî‚îÄ Temporal (40%): 228 muestras
        ‚îÇ
        ‚îú‚îÄ Validaci√≥n (20% del total): 114 muestras
        ‚îÇ
        ‚îî‚îÄ Prueba (20% del total): 114 muestras
```

#### C√≥digo de Divisi√≥n
```python
# Primera divisi√≥n: 60-40
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.4, random_state=RANDOM_SEED, stratify=y
)

# Segunda divisi√≥n: 20-20
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=RANDOM_SEED, stratify=y_temp
)
```

**Par√°metros Clave**:
- `stratify=y`: Mantiene la proporci√≥n de clases en cada conjunto (37%/63%)
- `random_state=RANDOM_SEED`: Asegura divisiones reproducibles
- `test_size`: Controla el tama√±o relativo de cada conjunto

**Importancia de la Estratificaci√≥n**: Con desbalanceo de clases, divisiones aleatorias pueden generar conjuntos no representativos. La estratificaci√≥n asegura que train, val y test tengan la misma distribuci√≥n de clases que el dataset original.

### Estandarizaci√≥n

#### Motivaci√≥n
Las caracter√≠sticas tienen escalas muy diferentes:
- √Årea: rango ~150-2500
- Suavidad: rango ~0.05-0.16
- Dimensi√≥n fractal: rango ~0.05-0.10

Sin estandarizaci√≥n, caracter√≠sticas con mayor magnitud dominar√≠an el gradiente durante el entrenamiento.

#### M√©todo: StandardScaler
Transforma cada caracter√≠stica a media 0 y desviaci√≥n est√°ndar 1:

$$z = \frac{x - \mu}{\sigma}$$

donde:
- $x$: valor original
- $\mu$: media de la caracter√≠stica en conjunto de entrenamiento
- $\sigma$: desviaci√≥n est√°ndar en conjunto de entrenamiento

#### C√≥digo
```python
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Calcula Œº y œÉ, transforma
X_val_scaled = scaler.transform(X_val)          # Solo transforma
X_test_scaled = scaler.transform(X_test)        # Solo transforma
```

**CR√çTICO - Prevenci√≥n de Data Leakage**:
- El scaler se **ajusta solo con datos de entrenamiento**
- Validaci√≥n y prueba se transforman con par√°metros ($\mu$, $\sigma$) de entrenamiento
- Usar estad√≠sticas de val/test para normalizar ser√≠a **data leakage** (informaci√≥n del futuro)

### Conversi√≥n a Tensores de PyTorch

```python
X_train_t = torch.FloatTensor(X_train_scaled)  # Caracter√≠sticas (float32)
y_train_t = torch.LongTensor(y_train)          # Etiquetas (int64)
```

**Tipos de Datos**:
- `FloatTensor`: Para caracter√≠sticas (valores continuos)
- `LongTensor`: Para etiquetas (√≠ndices de clase)

### DataLoaders

```python
train_loader = DataLoader(
    TensorDataset(X_train_t, y_train_t),
    batch_size=32,
    shuffle=True,      # Aleatorizar orden
    drop_last=True     # Descartar √∫ltimo batch incompleto
)
```

**Par√°metros**:
- `batch_size=32`: N√∫mero de muestras procesadas simult√°neamente
  - Muy peque√±o: entrenamiento lento, alta varianza
  - Muy grande: mucha memoria, convergencia pobre
  - 32 es un balance est√°ndar para datasets peque√±os
- `shuffle=True`: Aleatoriza orden en cada √©poca (solo train)
  - Previene que el modelo aprenda del orden de los datos
- `drop_last=True`: Descarta √∫ltimo batch si no es completo (solo train)
  - Asegura todos los batches tengan mismo tama√±o (importante para BatchNorm)

---

## Arquitecturas de Modelos

### 1. Baseline MLP (Multi-Layer Perceptron)

#### Arquitectura
```
Entrada (30) ‚Üí [144] ‚Üí [96] ‚Üí [48] ‚Üí [24] ‚Üí Salida (2)
```

#### Componentes por Bloque
Cada bloque contiene:
1. **Linear Layer**: Transformaci√≥n af√≠n $y = Wx + b$
2. **BatchNorm1d**: Normalizaci√≥n por batch
3. **GELU Activation**: Funci√≥n de activaci√≥n suave
4. **Dropout**: Regularizaci√≥n estoc√°stica

#### C√≥digo Completo con Documentaci√≥n

```python
class BaselineMLP(nn.Module):
    """
    Red Neuronal Profunda Multi-Capa (MLP) de referencia.
    
    Arquitectura:
        - 4 capas ocultas con dimensiones decrecientes
        - BatchNormalization para estabilidad
        - GELU como funci√≥n de activaci√≥n
        - Dropout para regularizaci√≥n
    
    Par√°metros:
        input_size (int): N√∫mero de caracter√≠sticas de entrada (default: 30)
        num_classes (int): N√∫mero de clases de salida (default: 2)
        dropout_rate (float): Tasa de dropout (default: 0.25)
    """
    def __init__(self, input_size=30, num_classes=2, dropout_rate=0.25):
        super().__init__()
        
        # Bloque 1: 30 ‚Üí 144
        self.fc1 = nn.Linear(input_size, 144)
        self.bn1 = nn.BatchNorm1d(144)
        
        # Bloque 2: 144 ‚Üí 96
        self.fc2 = nn.Linear(144, 96)
        self.bn2 = nn.BatchNorm1d(96)
        
        # Bloque 3: 96 ‚Üí 48
        self.fc3 = nn.Linear(96, 48)
        self.bn3 = nn.BatchNorm1d(48)
        
        # Bloque 4: 48 ‚Üí 24
        self.fc4 = nn.Linear(48, 24)
        self.bn4 = nn.BatchNorm1d(24)
        
        # Capa de salida: 24 ‚Üí 2
        self.output = nn.Linear(24, num_classes)
        
        # Funci√≥n de activaci√≥n y regularizaci√≥n
        self.activation = nn.GELU()  # Gaussian Error Linear Unit
        self.dropout = nn.Dropout(dropout_rate)
    
    def forward(self, x):
        """
        Propagaci√≥n hacia adelante.
        
        Par√°metros:
            x (torch.Tensor): Tensor de entrada con forma (batch_size, 30)
        
        Retorna:
            torch.Tensor: Logits de salida con forma (batch_size, 2)
        """
        # Bloque 1
        x = self.fc1(x)          # Transformaci√≥n lineal
        x = self.bn1(x)          # Normalizaci√≥n de batch
        x = self.activation(x)   # Funci√≥n de activaci√≥n
        x = self.dropout(x)      # Dropout para regularizaci√≥n
        
        # Bloque 2
        x = self.fc2(x)
        x = self.bn2(x)
        x = self.activation(x)
        x = self.dropout(x)
        
        # Bloque 3
        x = self.fc3(x)
        x = self.bn3(x)
        x = self.activation(x)
        x = self.dropout(x)
        
        # Bloque 4
        x = self.fc4(x)
        x = self.bn4(x)
        x = self.activation(x)
        x = self.dropout(x)
        
        # Capa de salida (sin activaci√≥n, se aplica en la funci√≥n de p√©rdida)
        return self.output(x)
```

#### Explicaci√≥n de Componentes

**1. Linear Layer (`nn.Linear`)**
- Implementa: $y = Wx + b$
- $W$: Matriz de pesos (entrenables)
- $b$: Vector de bias (entrenable)
- Ejemplo: `nn.Linear(30, 144)` tiene 30√ó144 + 144 = 4,464 par√°metros

**2. Batch Normalization (`nn.BatchNorm1d`)**
- Normaliza activaciones por batch:
  $$\hat{x} = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}}$$
- Luego escala y desplaza: $y = \gamma \hat{x} + \beta$
- **Ventajas**:
  - Estabiliza entrenamiento
  - Permite learning rates m√°s altos
  - Act√∫a como regularizador d√©bil
- **Par√°metros**: $\gamma$ (escala) y $\beta$ (desplazamiento) son entrenables

**3. GELU Activation (`nn.GELU`)**
- Gaussian Error Linear Unit: $GELU(x) = x \cdot \Phi(x)$
- $\Phi(x)$: Funci√≥n de distribuci√≥n acumulativa gaussiana
- **Ventajas sobre ReLU**:
  - Derivada suave (mejor para optimizaci√≥n)
  - No corta completamente valores negativos
  - Estado del arte en Transformers

**4. Dropout (`nn.Dropout`)**
- Durante entrenamiento: desactiva neuronas aleatoriamente con probabilidad $p$
- Durante inferencia: usa todas las neuronas, escala salidas por $(1-p)$
- **Prop√≥sito**: Prevenir overfitting forzando redundancia

---

### 2. Chebyshev-KAN

#### Fundamento Matem√°tico

**Polinomios de Chebyshev**:
Los polinomios de Chebyshev $T_n(x)$ son definidos en $[-1, 1]$ por:
- $T_0(x) = 1$
- $T_1(x) = x$
- $T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x)$ (recurrencia)

**Propiedades**:
1. **Ortogonales**: $\int_{-1}^{1} \frac{T_m(x)T_n(x)}{\sqrt{1-x^2}}dx = 0$ si $m \neq n$
2. **Mejor aproximaci√≥n**: Minimizan el error m√°ximo de aproximaci√≥n
3. **Rango acotado**: $|T_n(x)| \leq 1$ para $x \in [-1,1]$

**Aplicaci√≥n en KAN**:
Aproximar funci√≥n univariable $f(x)$ como:
$$f(x) \approx \sum_{n=0}^{N} a_n T_n(x)$$

#### Arquitectura Chebyshev-KAN

**ChebyshevBasis Layer**:
```python
class ChebyshevBasis(nn.Module):
    """
    Capa de transformaci√≥n basada en polinomios de Chebyshev.
    
    Aproxima funciones no-lineales univariables usando expansi√≥n en
    polinomios de Chebyshev ortogonales.
    
    Par√°metros:
        in_features (int): Dimensi√≥n de entrada
        out_features (int): Dimensi√≥n de salida
        degree (int): Grado m√°ximo del polinomio (default: 3)
    """
    def __init__(self, in_features, out_features, degree=3):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.degree = degree
        
        # Coeficientes para cada t√©rmino polinomial
        # Forma: (out_features, in_features, degree+1)
        self.coeffs = nn.Parameter(
            torch.randn(out_features, in_features, degree + 1) * 0.04
        )
        
        # Inicializaci√≥n Xavier para mejor convergencia
        nn.init.xavier_uniform_(self.coeffs.view(out_features, -1).unsqueeze(0))
        
        # T√©rmino de bias aprendible
        self.bias = nn.Parameter(torch.zeros(out_features))
        
        # Escala de salida aprendible con clamping
        self.output_scale = nn.Parameter(torch.ones(out_features) * 0.4)
        
        # LayerNorm para normalizaci√≥n adicional
        self.layer_norm = nn.LayerNorm(out_features)
    
    def chebyshev_poly(self, x, n):
        """
        Calcula el n-√©simo polinomio de Chebyshev usando recurrencia.
        
        Recurrencia: T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x)
        
        Par√°metros:
            x (torch.Tensor): Entrada normalizada a [-1, 1]
            n (int): Grado del polinomio
        
        Retorna:
            torch.Tensor: T_n(x)
        """
        if n == 0:
            return torch.ones_like(x)
        elif n == 1:
            return x
        else:
            T_prev2 = torch.ones_like(x)
            T_prev1 = x
            for _ in range(2, n + 1):
                T_curr = 2 * x * T_prev1 - T_prev2
                T_prev2, T_prev1 = T_prev1, T_curr
            return T_prev1
    
    def forward(self, x):
        """
        Propagaci√≥n hacia adelante.
        
        Pasos:
        1. Normalizar entrada a [-1, 1]
        2. Calcular polinomios de Chebyshev hasta grado N
        3. Combinar linealmente con coeficientes aprendibles
        4. Aplicar escala y bias
        5. Normalizar salida
        
        Par√°metros:
            x (torch.Tensor): Entrada (batch_size, in_features)
        
        Retorna:
            torch.Tensor: Salida (batch_size, out_features)
        """
        batch_size = x.shape[0]
        
        # Normalizaci√≥n a [-1, 1] usando min-max
        x_min = x.min(dim=0, keepdim=True)[0]
        x_max = x.max(dim=0, keepdim=True)[0]
        x_range = x_max - x_min + 1e-6
        x_norm = 2 * (x - x_min) / x_range - 1
        x_norm = torch.clamp(x_norm, -0.98, 0.98)  # Evitar valores extremos
        
        # Inicializar salida
        output = torch.zeros(batch_size, self.out_features, device=x.device)
        
        # Sumar contribuci√≥n de cada polinomio con ponderaci√≥n decreciente
        for n in range(self.degree + 1):
            # Calcular T_n(x_norm)
            cheby_n = self.chebyshev_poly(x_norm, n)
            
            # Ponderaci√≥n decreciente para altos grados (reduce overfitting)
            weight = 1.0 / (1.0 + 0.1 * n)
            
            # Combinar: (batch, in) @ (out, in).T -> (batch, out)
            output += weight * torch.mm(cheby_n, self.coeffs[:, :, n].t())
        
        # Aplicar escala con clamping para estabilidad
        scale = torch.clamp(self.output_scale, 0.1, 2.0)
        output = output * scale.unsqueeze(0) + self.bias.unsqueeze(0)
        
        # Normalizaci√≥n de capa
        output = self.layer_norm(output)
        
        return output
```

**Flujo de Datos**:
1. Entrada: $(batch, in\_features)$
2. Normalizaci√≥n: cada feature mapeada a $[-1, 1]$
3. Para cada caracter√≠stica:
   - Calcular $T_0(x), T_1(x), ..., T_N(x)$
   - Combinar con coeficientes: $\sum_{n=0}^{N} a_n T_n(x)$
4. Sumar contribuciones, aplicar escala y bias
5. LayerNorm para normalizaci√≥n final
6. Salida: $(batch, out\_features)$

#### Mejoras Implementadas (V4)

**Problema Detectado**: Desbalance entre clases (Specificity 0.889)

**Soluciones**:
1. **Grado Uniforme (3)**: Antes usaba grados progresivos (3‚Üí3‚Üí2) que causaban sesgo
2. **Dropout Agresivo**: Incrementado a 0.30‚Üí0.35‚Üí0.40‚Üí0.35
3. **Arquitectura Reducida**: 128‚Üí80‚Üí48‚Üí24 (menos par√°metros)
4. **Escala Aprendible**: Rango [0.1, 2.0] con clamping
5. **LayerNorm**: Normalizaci√≥n adicional en cada capa
6. **Ponderaci√≥n Decreciente**: $w_n = 1/(1+0.1n)$ para reducir influencia de altos grados

---

### 3. Fourier-KAN

#### Fundamento Matem√°tico

**Series de Fourier**:
Cualquier funci√≥n peri√≥dica $f(x)$ puede representarse como:
$$f(x) = \frac{a_0}{2} + \sum_{n=1}^{\infty} [a_n \cos(n\omega x + \phi_n) + b_n \sin(n\omega x + \phi_n)]$$

donde:
- $a_n, b_n$: coeficientes de Fourier
- $\omega$: frecuencia fundamental
- $\phi_n$: fase

**Aplicaci√≥n en KAN**:
- Aproximar funciones usando arm√≥nicos truncados
- Frecuencias y fases aprendibles
- √ötil para patrones con componentes peri√≥dicas

#### Arquitectura Fourier-KAN V4

**FourierBasis Layer**:
```python
class FourierBasis(nn.Module):
    """
    Capa de transformaci√≥n basada en Series de Fourier.
    
    Aproxima funciones usando combinaci√≥n de senos y cosenos con
    frecuencias y fases aprendibles.
    
    Par√°metros:
        in_features (int): Dimensi√≥n de entrada
        out_features (int): Dimensi√≥n de salida
        n_harmonics (int): N√∫mero de arm√≥nicos (default: 8)
    """
    def __init__(self, in_features, out_features, n_harmonics=8):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.n_harmonics = n_harmonics
        
        # Coeficientes para t√©rminos seno y coseno
        self.a_coeffs = nn.Parameter(
            torch.randn(out_features, in_features, n_harmonics) * 0.15
        )
        self.b_coeffs = nn.Parameter(
            torch.randn(out_features, in_features, n_harmonics) * 0.15
        )
        
        # Frecuencias mixtas: lineales + logar√≠tmicas
        freq_linear = torch.linspace(0.5, 10, n_harmonics // 2)
        freq_log = torch.logspace(0, 1.3, n_harmonics - n_harmonics // 2)
        freq_init = torch.cat([freq_linear, freq_log]).unsqueeze(0).unsqueeze(0)
        self.freq = nn.Parameter(
            freq_init.expand(out_features, in_features, n_harmonics).clone()
        )
        
        # Fase uniforme en [-œÄ, œÄ]
        self.phase = nn.Parameter(
            torch.rand(out_features, in_features, n_harmonics) * 2 * np.pi - np.pi
        )
        
        # Bias DC component
        self.bias = nn.Parameter(torch.zeros(out_features))
        
        # Normalizaci√≥n por arm√≥nico
        self.norm_factor = nn.Parameter(
            torch.ones(out_features, n_harmonics) * 0.2
        )
    
    def forward(self, x):
        """
        Propagaci√≥n hacia adelante.
        
        Calcula: f(x) = bias + Œ£[norm_n * (a_n*cos(œâ_n*x + œÜ_n) + b_n*sin(œâ_n*x + œÜ_n))]
        
        Par√°metros:
            x (torch.Tensor): Entrada (batch_size, in_features)
        
        Retorna:
            torch.Tensor: Salida (batch_size, out_features)
        """
        batch_size = x.shape[0]
        
        # Expandir para broadcasting
        x_expanded = x.unsqueeze(1).unsqueeze(3)  # (batch, 1, in, 1)
        
        # Clamping de frecuencias para estabilidad
        freq = torch.clamp(self.freq.unsqueeze(0), 0.1, 50)
        phase = self.phase.unsqueeze(0)
        a_coeffs = self.a_coeffs.unsqueeze(0)
        b_coeffs = self.b_coeffs.unsqueeze(0)
        
        # Calcular √°ngulo: œâ*x + œÜ
        angle = freq * x_expanded + phase
        
        # T√©rminos seno y coseno
        cos_term = torch.cos(angle)
        sin_term = torch.sin(angle)
        
        # Combinaci√≥n lineal
        fourier_out = cos_term * a_coeffs + sin_term * b_coeffs
        
        # Normalizaci√≥n por arm√≥nico
        norm = self.norm_factor.unsqueeze(0).unsqueeze(2)
        fourier_out = fourier_out * norm
        
        # Sumar sobre features y arm√≥nicos
        output = fourier_out.sum(dim=(2, 3))
        
        # A√±adir bias
        output = output + self.bias.unsqueeze(0)
        
        return output
```

#### Mejoras Implementadas (V4)

**Problema Detectado**: Underfitting (Sensitivity 0.857, MCC 0.810)

**Soluciones**:
1. **Arquitectura 67% M√°s Profunda**: 5 capas Fourier (vs 3)
2. **M√°s Arm√≥nicos**: 8 en capas iniciales (vs 6)
3. **Frecuencias Mixtas**: Lineales [0.5-10] + Logar√≠tmicas [1-20]
4. **Coeficientes 3x M√°s Fuertes**: 0.15 (vs 0.05)
5. **Dropout Reducido**: 0.20‚Üí0.15‚Üí0.10 (prevenir underfitting)
6. **Bias DC**: T√©rmino constante adicional
7. **Normalizaci√≥n por Arm√≥nico**: Control individual de cada frecuencia

---

### 4. Wavelet-KAN

#### Fundamento Matem√°tico

**Wavelets (Mexican Hat / Ricker)**:
$$\psi(x) = (1 - x^2) e^{-x^2/2}$$

**Propiedades**:
- **Localizaci√≥n**: Activa solo en ventana estrecha
- **Captura caracter√≠sticas locales**: Bordes, texturas, discontinuidades
- **Escalable y trasladable**: $\psi_{a,b}(x) = \frac{1}{\sqrt{a}}\psi(\frac{x-b}{a})$

**Aplicaci√≥n en KAN**:
- Detectar patrones locales en caracter√≠sticas
- √ötil para dimensi√≥n fractal, puntos c√≥ncavos (cambios abruptos)

#### Arquitectura Wavelet-KAN V3

```python
class WaveletBasis(nn.Module):
    """
    Capa de transformaci√≥n basada en Wavelets Mexican Hat.
    
    Detecta caracter√≠sticas locales usando wavelets escalables y
    trasladables.
    
    Par√°metros:
        in_features (int): Dimensi√≥n de entrada
        out_features (int): Dimensi√≥n de salida
    """
    def __init__(self, in_features, out_features):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # Par√°metros de escala y traslaci√≥n
        self.scale = nn.Parameter(torch.ones(out_features, in_features) * 0.8)
        self.translation = nn.Parameter(torch.zeros(out_features, in_features))
        
        # Pesos aprendibles para combinar wavelets
        self.weights = nn.Parameter(torch.ones(out_features, in_features) * 0.5)
        
        # Inicializaci√≥n robusta
        nn.init.uniform_(self.scale, 0.3, 1.5)
        nn.init.uniform_(self.translation, -0.5, 0.5)
        nn.init.xavier_uniform_(self.weights.unsqueeze(0))
    
    def mexican_hat_wavelet(self, x):
        """
        Calcula Mexican Hat (Ricker) Wavelet.
        
        œà(x) = (1 - x¬≤) * exp(-x¬≤/2)
        
        Par√°metros:
            x (torch.Tensor): Entrada
        
        Retorna:
            torch.Tensor: Wavelet evaluada
        """
        return (1 - x**2) * torch.exp(-0.5 * x**2)
    
    def forward(self, x):
        """
        Propagaci√≥n hacia adelante.
        
        Pasos:
        1. Expandir entrada
        2. Aplicar escala y traslaci√≥n
        3. Evaluar wavelet
        4. Ponderar y combinar
        
        Par√°metros:
            x (torch.Tensor): Entrada (batch_size, in_features)
        
        Retorna:
            torch.Tensor: Salida (batch_size, out_features)
        """
        x_expanded = x.unsqueeze(1)  # (batch, 1, in_features)
        
        # Aplicar escala y traslaci√≥n
        scaled = (x_expanded - self.translation) / (torch.abs(self.scale) + 1e-5)
        
        # Evaluar wavelet
        wavelet_out = self.mexican_hat_wavelet(scaled)
        
        # Combinar con pesos aprendibles
        weighted_out = wavelet_out * torch.abs(self.weights)
        
        # Sumar sobre features
        return torch.sum(weighted_out, dim=2)
```

#### Mejoras Implementadas (V3)

**Problema Detectado**: Overfitting severo

**Soluciones**:
1. **Arquitectura M√°s Shallow**: 2 capas wavelet (vs 3)
2. **Neuronas Reducidas**: 96‚Üí64‚Üí32
3. **Dropout Agresivo**: 0.40‚Üí0.45‚Üí0.40
4. **Weight Decay Fuerte**: 0.05
5. **Pesos Aprendibles**: Para combinar wavelets

---

### 5. Fast-KAN (RBF)

#### Fundamento Matem√°tico

**Funciones de Base Radial (RBF) Gaussianas**:
$$\phi(x; c, \sigma) = \exp\left(-\frac{||x - c||^2}{2\sigma^2}\right)$$

donde:
- $c$: centro de la RBF
- $\sigma$: ancho (controla localizaci√≥n)

**Propiedades**:
- **Localizaci√≥n radial**: Activa en vecindad de centro
- **Suavidad**: Derivadas continuas infinitas
- **Aproximaci√≥n universal**: Puede aproximar cualquier funci√≥n continua

**Aplicaci√≥n en KAN**:
- M√∫ltiples centros distribuidos en espacio de entrada
- Anchos adaptativos por centro
- Combinaci√≥n lineal ponderada

#### Arquitectura Fast-KAN V3

```python
class RBFBasis(nn.Module):
    """
    Capa de transformaci√≥n basada en Funciones de Base Radial Gaussianas.
    
    Usa m√∫ltiples centros RBF fijos con anchos adaptativos.
    
    Par√°metros:
        in_features (int): Dimensi√≥n de entrada
        out_features (int): Dimensi√≥n de salida
        num_rbf_centers (int): N√∫mero de centros RBF (default: 8)
    """
    def __init__(self, in_features, out_features, num_rbf_centers=8):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.num_rbf_centers = num_rbf_centers
        
        # Centros RBF FIJOS (no entrenables) para prevenir drift
        centers_init = torch.linspace(-2, 2, num_rbf_centers)
        centers_init = centers_init.unsqueeze(0).unsqueeze(0).expand(
            out_features, in_features, num_rbf_centers
        ).clone()
        self.register_buffer('centers', centers_init)
        
        # Anchos adaptativos (log-parametrizaci√≥n para positividad)
        self.log_widths = nn.Parameter(
            torch.ones(out_features, in_features, num_rbf_centers) * 0.3
        )
        
        # Pesos de combinaci√≥n
        self.weights = nn.Parameter(
            torch.randn(out_features, in_features, num_rbf_centers) * 0.05
        )
        
        # LayerNorm para normalizaci√≥n estable
        self.layer_norm = nn.LayerNorm(out_features)
    
    def forward(self, x):
        """
        Propagaci√≥n hacia adelante.
        
        Calcula: f(x) = Œ£ w_i * exp(-||x - c_i||¬≤ / (2œÉ_i¬≤))
        
        Par√°metros:
            x (torch.Tensor): Entrada (batch_size, in_features)
        
        Retorna:
            torch.Tensor: Salida (batch_size, out_features)
        """
        batch_size = x.shape[0]
        
        # Expandir para broadcasting
        x_expanded = x.unsqueeze(1).unsqueeze(3)  # (batch, 1, in, 1)
        centers = self.centers.unsqueeze(0)        # (1, out, in, centers)
        
        # Distancia a centros
        diff = x_expanded - centers
        
        # Anchos con clamping fuerte para estabilidad
        widths = torch.exp(torch.clamp(self.log_widths, -1.5, 1.0))
        
        # RBF Gaussiana
        rbf_out = torch.exp(-0.5 * (diff / (widths.unsqueeze(0) + 1e-5))**2)
        
        # Combinar con pesos
        weighted_rbf = rbf_out * self.weights.unsqueeze(0)
        
        # Sumar sobre features y centros
        output = weighted_rbf.sum(dim=(2, 3))
        
        # LayerNorm
        output = self.layer_norm(output)
        
        return output
```

#### Mejoras Implementadas (V3)

**Problema Detectado**: Inestabilidad extrema

**Soluciones**:
1. **Centros RBF Fijos**: No entrenables (previene drift)
2. **Anchos Restringidos**: Clamp (-1.5, 1.0)
3. **LayerNorm**: Por capa
4. **Arquitectura Reducida**: 128‚Üí80‚Üí40‚Üí24
5. **Dropout Agresivo**: 0.30‚Üí0.35‚Üí0.40‚Üí0.35

---

## Proceso de Entrenamiento

### Framework de Entrenamiento Unificado

#### Funci√≥n de P√©rdida

**CrossEntropyLoss con Pesos de Clase**:
```python
class_weights = torch.tensor([1.0, 2.5]).to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights)
```

**Justificaci√≥n**:
- Peso 2.5x para clase maligna (1)
- Penaliza m√°s errores en malignos (falsos negativos)
- Balance entre sensitivity y specificity

#### Optimizador

**AdamW con Weight Decay Diferenciado**:
```python
optimizer = torch.optim.AdamW(
    model.parameters(), 
    lr=lr, 
    weight_decay=weight_decay
)
```

**Configuraci√≥n por Modelo**:
- **Modelos estables** (MLP, Fourier, Baseline): `weight_decay=0.01`
- **Modelos inestables** (Chebyshev): `weight_decay=0.03`
- **Modelos con overfitting** (Wavelet, Fast): `weight_decay=0.05`

#### Learning Rate Schedulers

**Estrategia Dual**:
```python
# 1. Cosine Annealing
scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=epochs-warmup, eta_min=lr*0.1
)

# 2. ReduceLROnPlateau
scheduler_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='max', factor=0.5, patience=8
)
```

**Warmup**: 10 √©pocas con LR lineal de 0 a `lr`

**Justificaci√≥n**:
- Cosine Annealing: Decaimiento suave y predecible
- ReduceLROnPlateau: Ajuste autom√°tico si se estanca

#### Gradient Clipping

**Diferenciado por Modelo**:
```python
# Modelos inestables
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)

# Modelos estables
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**Prop√≥sito**: Prevenir explosi√≥n de gradientes en KAN

#### Early Stopping

**Multi-M√©trica**:
```python
val_score = (
    val_metrics['sensitivity'] * 0.6 + 
    val_metrics['specificity'] * 0.3 + 
    val_metrics['f1_score'] * 0.1
)
```

**Ponderaciones**:
- 60% Sensitivity (prioridad en contexto m√©dico)
- 30% Specificity
- 10% F1-Score

**Paciencia**: 20-25 √©pocas seg√∫n estabilidad del modelo

---

### Funci√≥n de Entrenamiento Completa

```python
def train_and_evaluate(model, train_loader, val_loader, test_loader, 
                       model_name, epochs=150, lr=0.0008, device='cpu'):
    """
    Entrena y eval√∫a un modelo con configuraci√≥n optimizada.
    
    Par√°metros:
        model: Instancia del modelo a entrenar
        train_loader: DataLoader de entrenamiento
        val_loader: DataLoader de validaci√≥n
        test_loader: DataLoader de prueba
        model_name (str): Nombre del modelo para logging
        epochs (int): N√∫mero m√°ximo de √©pocas
        lr (float): Learning rate inicial
        device: Dispositivo de c√≥mputo (CPU/GPU)
    
    Retorna:
        dict: Diccionario con:
            - 'model': Modelo entrenado
            - 'history': Historial de m√©tricas por √©poca
            - 'test_metrics': M√©tricas en conjunto de prueba
            - 'test_predictions': Predicciones y probabilidades
    """
    print(f"\n{'='*60}")
    print(f"Entrenando: {model_name}")
    print(f"{'='*60}")
    
    model = model.to(device)
    
    # Configuraci√≥n espec√≠fica por modelo
    if 'Wavelet' in model_name or 'Fast' in model_name:
        weight_decay = 0.05
        clip_norm = 0.5
        patience = 25
    elif 'Chebyshev' in model_name:
        weight_decay = 0.03
        clip_norm = 0.7
        patience = 20
    else:
        weight_decay = 0.01
        clip_norm = 1.0
        patience = 20
    
    # Funci√≥n de p√©rdida con pesos
    class_weights = torch.tensor([1.0, 2.5]).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    # Optimizador
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=lr, weight_decay=weight_decay
    )
    
    # Schedulers
    warmup_epochs = 10
    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=epochs - warmup_epochs, eta_min=lr * 0.1
    )
    scheduler_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=8, min_lr=lr * 0.01
    )
    
    # Early stopping
    best_val_score = -float('inf')
    patience_counter = 0
    best_model_state = None
    
    # Historial
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_sensitivity': [],
        'val_specificity': [],
        'learning_rate': []
    }
    
    # Loop de entrenamiento
    for epoch in range(epochs):
        # Warmup
        if epoch < warmup_epochs:
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr * (epoch + 1) / warmup_epochs
        
        # Entrenamiento
        train_loss = train_epoch(
            model, train_loader, criterion, optimizer, device, clip_norm
        )
        
        # Validaci√≥n
        y_true, y_pred, y_prob = evaluate_model(model, val_loader, device)
        val_metrics = calculate_clinical_metrics(y_true, y_pred, y_prob)
        
        # Calcular val_loss
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                val_loss += criterion(outputs, y_batch).item()
        val_loss /= len(val_loader)
        
        # Guardar historial
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_sensitivity'].append(val_metrics['sensitivity'])
        history['val_specificity'].append(val_metrics['specificity'])
        history['learning_rate'].append(optimizer.param_groups[0]['lr'])
        
        # Score combinado
        val_score = (
            val_metrics['sensitivity'] * 0.6 + 
            val_metrics['specificity'] * 0.3 + 
            val_metrics['f1_score'] * 0.1
        )
        
        # Early stopping
        if val_score > best_val_score:
            best_val_score = val_score
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
        
        # Schedulers
        if epoch >= warmup_epochs:
            scheduler_cosine.step()
            scheduler_plateau.step(val_score)
        
        # Logging
        if (epoch + 1) % 15 == 0:
            print(f"Epoch {epoch+1}/{epochs} | "
                  f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | "
                  f"Sensitivity: {val_metrics['sensitivity']:.4f} | "
                  f"Specificity: {val_metrics['specificity']:.4f} | "
                  f"LR: {optimizer.param_groups[0]['lr']:.6f}")
        
        # Early stopping
        if patience_counter >= patience:
            print(f"Early stopping en √©poca {epoch+1}")
            break
    
    # Cargar mejor modelo
    model.load_state_dict(best_model_state)
    
    # Evaluaci√≥n final en test
    y_true_test, y_pred_test, y_prob_test = evaluate_model(model, test_loader, device)
    test_metrics = calculate_clinical_metrics(y_true_test, y_pred_test, y_prob_test)
    
    print(f"\nResultados en Test Set:")
    print(f"   Sensitivity: {test_metrics['sensitivity']:.4f}")
    print(f"   Specificity: {test_metrics['specificity']:.4f}")
    print(f"   F1-Score: {test_metrics['f1_score']:.4f}")
    print(f"   AUC-ROC: {test_metrics['auc_roc']:.4f}")
    print(f"   MCC: {test_metrics['mcc']:.4f}")
    
    return {
        'model': model,
        'history': history,
        'test_metrics': test_metrics,
        'test_predictions': (y_true_test, y_pred_test, y_prob_test)
    }
```

---

## Evaluaci√≥n y M√©tricas

### M√©tricas Cl√≠nicas

**Matriz de Confusi√≥n**:
```
                Predicci√≥n
              Benigno  Maligno
Real Benigno    TN       FP
     Maligno    FN       TP
```

**M√©tricas Derivadas**:

1. **Sensitivity (Sensibilidad / Recall)**:
   $$Sensitivity = \frac{TP}{TP + FN}$$
   - Proporci√≥n de malignos correctamente identificados
   - **Cr√≠tico en medicina**: Minimizar FN

2. **Specificity (Especificidad)**:
   $$Specificity = \frac{TN}{TN + FP}$$
   - Proporci√≥n de benignos correctamente identificados

3. **Positive Predictive Value (PPV / Precision)**:
   $$PPV = \frac{TP}{TP + FP}$$
   - De los predichos malignos, cu√°ntos son realmente malignos

4. **Negative Predictive Value (NPV)**:
   $$NPV = \frac{TN}{TN + FN}$$
   - De los predichos benignos, cu√°ntos son realmente benignos
   - **Cr√≠tico en screening**: Alta NPV permite confiar en negativo

5. **F1-Score**:
   $$F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$$
   - Media arm√≥nica entre precision y recall

6. **Matthews Correlation Coefficient (MCC)**:
   $$MCC = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$
   - Rango: [-1, 1]
   - M√©trica m√°s balanceada para clases desbalanceadas

7. **AUC-ROC (Area Under ROC Curve)**:
   - Curva ROC: Sensitivity vs (1 - Specificity)
   - AUC: Probabilidad de que modelo ordene aleatorios positivo y negativo correctamente
   - Rango: [0, 1], 0.5 = aleatorio, 1.0 = perfecto

---

### Implementaci√≥n de C√°lculo de M√©tricas

```python
def calculate_clinical_metrics(y_true, y_pred, y_prob):
    """
    Calcula m√©tricas cl√≠nicas completas para evaluaci√≥n m√©dica.
    
    Par√°metros:
        y_true (np.array): Etiquetas verdaderas
        y_pred (np.array): Predicciones del modelo
        y_prob (np.array): Probabilidades de clase positiva
    
    Retorna:
        dict: Diccionario con todas las m√©tricas cl√≠nicas
    """
    # Matriz de confusi√≥n
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    
    # M√©tricas cl√≠nicas
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0
    
    # M√©tricas est√°ndar
    f1 = f1_score(y_true, y_pred)
    auc_roc = roc_auc_score(y_true, y_prob)
    mcc = matthews_corrcoef(y_true, y_pred)
    
    return {
        'sensitivity': sensitivity,
        'specificity': specificity,
        'ppv': ppv,
        'npv': npv,
        'f1_score': f1,
        'auc_roc': auc_roc,
        'mcc': mcc,
        'tn': int(tn), 'fp': int(fp),
        'fn': int(fn), 'tp': int(tp)
    }
```

---

## An√°lisis de Resultados

### Resultados Finales (Versiones Optimizadas)

Los siguientes resultados corresponden al entrenamiento final con las arquitecturas optimizadas (V3/V4) en el conjunto de prueba (20% del dataset, 114 muestras):

| Modelo | Sensitivity | Specificity | F1-Score | AUC-ROC | MCC | FN | FP |
|--------|------------|-------------|----------|---------|-----|----|----|
| **Baseline MLP** | **0.9524** | **1.0000** | **0.9756** | 0.9954 | **0.9626** | 2 | 0 |
| **Chebyshev-KAN V4** | **1.0000** | 0.9444 | 0.9545 | **0.9980** | 0.9286 | 0 | 4 |
| **Wavelet-KAN V3** | 0.8571 | **0.9861** | 0.9114 | 0.9792 | 0.8688 | 6 | 1 |
| **Fast-KAN V3** | 0.9762 | 0.9722 | 0.9647 | 0.9874 | 0.9439 | 1 | 2 |
| **Fourier-KAN V4** | 0.8571 | 0.9167 | 0.8571 | 0.9649 | 0.7738 | 6 | 6 |

#### Interpretaci√≥n Cl√≠nica de los Resultados

**Matriz de Confusi√≥n - Contexto M√©dico**:
- **TP (True Positives)**: Casos malignos correctamente detectados ‚Üí Pacientes reciben tratamiento adecuado
- **TN (True Negatives)**: Casos benignos correctamente identificados ‚Üí Evita biopsias innecesarias
- **FN (False Negatives)**: Casos malignos no detectados ‚Üí **MUY CR√çTICO** - Pacientes con c√°ncer no reciben tratamiento
- **FP (False Positives)**: Casos benignos clasificados como malignos ‚Üí Genera biopsias adicionales (menos cr√≠tico)

### An√°lisis Detallado por Modelo

#### 1. Baseline MLP (Mejor Balance General)
```
M√©tricas Clave:
- Sensitivity: 0.9524 (95.2% de malignos detectados)
- Specificity: 1.0000 (100% de benignos correctos)
- MCC: 0.9626 (mejor correlaci√≥n global)
- Falsos Negativos: 2 (solo 2 casos malignos perdidos)
- Falsos Positivos: 0 (ning√∫n benigno mal clasificado)
```

**Fortalezas**:
- **Mejor MCC global** (0.9626) indica balance √≥ptimo
- **Specificity perfecta** (1.0000) - ning√∫n falso positivo
- Arquitectura madura y estable
- Menor complejidad computacional que KAN

**Limitaciones**:
- 2 falsos negativos (cr√≠tico en detecci√≥n de c√°ncer)
- Menor interpretabilidad que KAN

**Conclusi√≥n**: Excelente opci√≥n para **producci√≥n** por su balance y estabilidad.

---

#### 2. Chebyshev-KAN V4 (Mejor para Screening)
```
M√©tricas Clave:
- Sensitivity: 1.0000 (100% de malignos detectados)
- Specificity: 0.9444 (94.4% de benignos correctos)
- AUC-ROC: 0.9980 (mejor discriminaci√≥n)
- Falsos Negativos: 0 (ning√∫n caso maligno perdido)
- Falsos Positivos: 4 (4 casos benignos mal clasificados)
```

**Fortalezas**:
- **Sensitivity perfecta** (1.0000) - **CR√çTICO para screening**
- **Mejor AUC-ROC** (0.9980) - excelente capacidad discriminativa
- Cero falsos negativos (ning√∫n paciente con c√°ncer no detectado)
- Interpretabilidad mediante an√°lisis de coeficientes polinomiales

**Limitaciones**:
- 4 falsos positivos (4 pacientes sin c√°ncer recibir√≠an biopsias innecesarias)
- Mayor complejidad computacional que MLP
- Specificity 94.4% (inferior a MLP)

**Conclusi√≥n**: **Ideal para screening masivo** donde el objetivo es no perder ning√∫n caso de c√°ncer, aceptando algunos falsos positivos.

**Interpretaci√≥n de Mejoras V4**:
- Grado uniforme (3) + LayerNorm ‚Üí Mayor estabilidad
- Dropout agresivo (0.30-0.40) ‚Üí Previene overfitting en clase maligna
- Escala aprendible ‚Üí Adaptaci√≥n autom√°tica a magnitud de caracter√≠sticas

---

#### 3. Wavelet-KAN V3 (Especialista en Patrones Locales)
```
M√©tricas Clave:
- Sensitivity: 0.8571 (85.7% de malignos detectados)
- Specificity: 0.9861 (98.6% de benignos correctos)
- MCC: 0.8688
- Falsos Negativos: 6 (6 casos malignos no detectados)
- Falsos Positivos: 1 (1 caso benigno mal clasificado)
```

**Fortalezas**:
- **Alta Specificity** (0.9861) - muy pocos falsos positivos
- Excelente para detectar caracter√≠sticas locales (fractal dimension, concavidad)
- Solo 1 falso positivo
- √ötil cuando se prioriza especificidad sobre sensibilidad

**Limitaciones**:
- **Sensitivity baja** (0.8571) - 6 casos malignos no detectados
- **No recomendado para screening** por alta tasa de falsos negativos
- Requiere ajuste adicional para aplicaci√≥n cl√≠nica

**Conclusi√≥n**: √ötil como **modelo complementario** en ensemble para detectar patrones espec√≠ficos (texturas, irregularidades).

**Interpretaci√≥n de Mejoras V3**:
- Arquitectura shallow (2 capas) ‚Üí Reduce overfitting
- Dropout agresivo (0.40-0.45) ‚Üí Estabilizaci√≥n forzada
- Wavelets Mexican Hat ‚Üí Detecta cambios abruptos en caracter√≠sticas

---

#### 4. Fast-KAN V3 (Balance Robusto)
```
M√©tricas Clave:
- Sensitivity: 0.9762 (97.6% de malignos detectados)
- Specificity: 0.9722 (97.2% de benignos correctos)
- MCC: 0.9439
- Falsos Negativos: 1 (1 caso maligno no detectado)
- Falsos Positivos: 2 (2 casos benignos mal clasificados)
```

**Fortalezas**:
- **Excelente balance** Sensitivity/Specificity (~97% ambos)
- Solo 1 falso negativo (segundo mejor en detecci√≥n de malignos)
- RBF con centros fijos ‚Üí Estabilidad mejorada dram√°ticamente
- Buen desempe√±o en espacios de alta dimensi√≥n (30 features)

**Limitaciones**:
- No destaca en ninguna m√©trica espec√≠fica
- Complejidad computacional moderada-alta

**Conclusi√≥n**: Opci√≥n **robusta y balanceada** para aplicaciones cl√≠nicas donde se requiere equilibrio entre sensitivity y specificity.

**Interpretaci√≥n de Mejoras V3**:
- Centros RBF fijos ‚Üí Previene drift durante entrenamiento
- Anchos con clamp (-1.5, 1.0) ‚Üí Estabilizaci√≥n de gaussianas
- LayerNorm ‚Üí Normalizaci√≥n adicional para convergencia

---

#### 5. Fourier-KAN V4 (Necesita Optimizaci√≥n Adicional)
```
M√©tricas Clave:
- Sensitivity: 0.8571 (85.7% de malignos detectados)
- Specificity: 0.9167 (91.7% de benignos correctos)
- MCC: 0.7738 (menor correlaci√≥n)
- Falsos Negativos: 6 (6 casos malignos no detectados)
- Falsos Positivos: 6 (6 casos benignos mal clasificados)
```

**Fortalezas**:
- Arquitectura m√°s profunda (5 capas) con mayor expresividad
- Frecuencias mixtas pueden capturar patrones complejos
- Potencial para capturar simetr√≠a celular

**Limitaciones**:
- **Peor desempe√±o general** en este dataset
- 6 falsos negativos + 6 falsos positivos (12 errores totales)
- **No recomendado para aplicaci√≥n cl√≠nica** en su estado actual
- Posiblemente requiere m√°s datos o ajustes adicionales

**Conclusi√≥n**: A pesar de las mejoras V4, **Fourier-KAN no logra desempe√±o competitivo** en este dataset. Posibles razones:
1. Caracter√≠sticas del dataset no tienen patrones peri√≥dicos fuertes
2. Arquitectura muy profunda puede estar causando overfitting residual
3. Necesita exploraci√≥n adicional de hiperpar√°metros

**Recomendaci√≥n**: Considerar arquitectura h√≠brida (Fourier + Chebyshev) o explorar datasets con patrones m√°s peri√≥dicos.

---

### Comparaci√≥n Visual de Trade-offs

```
PRIORIDAD: SENSITIVITY (Detectar C√°ncer)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Chebyshev-KAN V4:  1.0000 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (MEJOR - 0 FN)
Fast-KAN V3:       0.9762 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë
Baseline MLP:      0.9524 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë
Wavelet-KAN V3:    0.8571 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë
Fourier-KAN V4:    0.8571 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë

PRIORIDAD: SPECIFICITY (Evitar Biopsias Innecesarias)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Baseline MLP:      1.0000 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (PERFECTO)
Wavelet-KAN V3:    0.9861 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë
Fast-KAN V3:       0.9722 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë
Chebyshev-KAN V4:  0.9444 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë
Fourier-KAN V4:    0.9167 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë

PRIORIDAD: BALANCE (MCC)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Baseline MLP:      0.9626 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (MEJOR)
Fast-KAN V3:       0.9439 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë
Chebyshev-KAN V4:  0.9286 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë
Wavelet-KAN V3:    0.8688 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë
Fourier-KAN V4:    0.7738 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë
```

### Ranking Final por Aplicaci√≥n Cl√≠nica

#### ü•á Para Screening Masivo (Prioridad: Detectar TODO c√°ncer)
**Recomendaci√≥n**: **Chebyshev-KAN V4**
- Sensitivity: 1.0000 (0 falsos negativos)
- AUC-ROC: 0.9980 (mejor discriminaci√≥n)
- Trade-off aceptable: 4 falsos positivos ‚Üí biopsias adicionales
- **Impacto**: Ning√∫n paciente con c√°ncer queda sin diagnosticar

#### ü•à Para Diagn√≥stico de Confirmaci√≥n (Balance Sensitivity-Specificity)
**Recomendaci√≥n**: **Baseline MLP** o **Fast-KAN V3**
- Baseline MLP: Specificity perfecta, solo 2 FN
- Fast-KAN V3: Balance 97.6%/97.2%, solo 1 FN
- **Impacto**: Minimiza errores en ambas direcciones

#### ü•â Para Producci√≥n (Estabilidad y Robustez)
**Recomendaci√≥n**: **Baseline MLP**
- MCC: 0.9626 (mejor balance)
- Arquitectura madura y probada
- Menor complejidad computacional
- **Impacto**: Despliegue confiable y mantenible

#### üî¨ Para Investigaci√≥n (Interpretabilidad)
**Recomendaci√≥n**: **Chebyshev-KAN V4**
- Coeficientes polinomiales interpretables
- AUC-ROC: 0.9980
- An√°lisis de contribuci√≥n por caracter√≠stica
- **Impacto**: Insights sobre relaciones no-lineales entre features

---

## Conclusiones

### Hallazgos Principales

#### 1. KAN vs MLP: Rendimiento Comparable con Trade-offs Espec√≠ficos
- **Baseline MLP**: Mejor MCC (0.9626) y Specificity perfecta (1.0000)
- **Chebyshev-KAN V4**: Mejor AUC-ROC (0.9980) y Sensitivity perfecta (1.0000)
- **Fast-KAN V3**: Mejor balance general (Sens: 0.9762, Spec: 0.9722)

**Conclusi√≥n**: Las redes KAN **no superan universalmente** a MLP tradicional, pero ofrecen **trade-offs valiosos** seg√∫n la aplicaci√≥n.

#### 2. La Arquitectura Debe Adaptarse al Problema
- **Chebyshev-KAN**: Sobresale con caracter√≠sticas polinomiales suaves (radius, area, perimeter)
- **Wavelet-KAN**: Mejor para patrones locales (fractal dimension, concavidad) pero underfitting en este dataset
- **Fourier-KAN**: Bajo desempe√±o sugiere que caracter√≠sticas no tienen patrones peri√≥dicos fuertes
- **Fast-KAN**: Balance robusto gracias a centros RBF fijos en espacios de alta dimensi√≥n

**Conclusi√≥n**: La **elecci√≥n de base funcional** debe estar guiada por el **dominio del problema**.

#### 3. Dropout es Cr√≠tico para Regularizaci√≥n
- **Muy alto** (0.40-0.45 en Wavelet): Caus√≥ underfitting ‚Üí Sensitivity 0.8571
- **Muy bajo** (0.10-0.20 en Fourier inicial): Caus√≥ overfitting ‚Üí Inestabilidad
- **√ìptimo** (0.30-0.35 en Chebyshev y Fast): Balance entre regularizaci√≥n y capacidad

**Conclusi√≥n**: Dropout debe **ajustarse por arquitectura** seg√∫n complejidad y tendencia al overfitting.

#### 4. Sensitivity Perfecta es Alcanzable pero con Trade-offs
- **Chebyshev-KAN V4**: Sensitivity 1.0000 (0 FN) pero 4 falsos positivos
- **Baseline MLP**: Specificity 1.0000 (0 FP) pero 2 falsos negativos

**Conclusi√≥n**: En aplicaciones m√©dicas, priorizar **Sensitivity** (detectar c√°ncer) aceptando m√°s falsos positivos es generalmente **preferible** a priorizar Specificity.

#### 5. AUC-ROC Alto No Garantiza Bajo Error Pr√°ctico
- **Chebyshev-KAN V4**: AUC 0.9980 pero 4 FP
- **Fourier-KAN V4**: AUC 0.9649 pero 12 errores totales (6 FN + 6 FP)

**Conclusi√≥n**: AUC-ROC mide **capacidad de discriminaci√≥n global**, no necesariamente **errores cl√≠nicos m√≠nimos** en umbral operativo.

#### 6. Estabilizaci√≥n de RBF Requiere Centros Fijos
**Fast-KAN antes de V3**: Especificidad catastr√≥fica (~0.25) por drift de centros durante entrenamiento.

**Fast-KAN V3**: Centros fijos + anchos con clamp ‚Üí Sensitivity 0.9762, Specificity 0.9722.

**Conclusi√≥n**: En arquitecturas basadas en RBF, **anclar centros** es esencial para estabilidad.

#### 7. Interpretabilidad vs Desempe√±o: KAN Ofrece Ventaja
- **MLP**: Mejor MCC pero pesos sin interpretaci√≥n directa
- **Chebyshev-KAN**: Coeficientes polinomiales revelan relaciones no-lineales entre features
- **Wavelet-KAN**: Escalas y traslaciones indican localizaci√≥n de patrones discriminativos

**Conclusi√≥n**: KAN permite **an√°lisis post-hoc** de qu√© transformaciones funcionales son importantes.

#### 8. Profundidad vs Regularizaci√≥n: Balance Delicado
- **Fourier-KAN V4**: 5 capas profundas para combatir underfitting inicial
- **Wavelet-KAN V3**: Solo 2 capas wavelet para combatir overfitting
- **Chebyshev-KAN V4**: 4 capas con dropout agresivo progresivo

**Conclusi√≥n**: Profundidad debe **aumentarse para underfitting** pero **acompa√±arse de regularizaci√≥n fuerte** (dropout, weight decay).

---

### Recomendaciones para Producci√≥n

#### Escenario 1: Screening Masivo (Hospitales, Campa√±as)
**Modelo Recomendado**: **Chebyshev-KAN V4**

**Justificaci√≥n**:
- Sensitivity perfecta (1.0000) ‚Üí Ning√∫n caso de c√°ncer pasa desapercibido
- AUC-ROC m√°s alto (0.9980) ‚Üí Mejor discriminaci√≥n global
- 4 falsos positivos ‚Üí Costo aceptable (biopsias adicionales) vs beneficio (salvar vidas)

**Protocolo de Implementaci√≥n**:
1. Usar Chebyshev-KAN V4 como **primera l√≠nea de screening**
2. Casos positivos ‚Üí Enviar a biopsia y pruebas adicionales
3. Monitoreo continuo de tasa de biopsias innecesarias

**M√©tricas de √âxito**:
- **Sensitivity > 99%** (permitir m√°ximo 1% de falsos negativos)
- Tasa de biopsia innecesaria < 10%
- NPV (Negative Predictive Value) > 99.5%

---

#### Escenario 2: Diagn√≥stico de Confirmaci√≥n (Cl√≠nicas Especializadas)
**Modelo Recomendado**: **Baseline MLP** o **Fast-KAN V3**

**Justificaci√≥n**:
- **Baseline MLP**: Specificity perfecta (1.0000), MCC 0.9626
- **Fast-KAN V3**: Balance 97.6%/97.2%, solo 1 FN
- Ambos minimizan errores totales

**Protocolo de Implementaci√≥n**:
1. Pacientes con resultados preliminares positivos ‚Üí Evaluaci√≥n con MLP
2. Si MLP confirma maligno ‚Üí Tratamiento inmediato
3. Si MLP indica benigno pero screening fue positivo ‚Üí Pruebas adicionales

**M√©tricas de √âxito**:
- **Balance Sensitivity-Specificity > 95%** en ambas
- MCC > 0.93
- Tasa de error total < 5%

---

#### Escenario 3: Producci√≥n (Sistemas Hospitalarios)
**Modelo Recomendado**: **Baseline MLP**

**Justificaci√≥n**:
- Arquitectura madura y probada
- Menor complejidad computacional ‚Üí Latencia baja
- MCC m√°s alto (0.9626) ‚Üí Mejor balance general
- Facilidad de mantenimiento y actualizaci√≥n

**Protocolo de Implementaci√≥n**:
1. Despliegue en servidores con PyTorch optimizado
2. API REST con tiempos de respuesta < 100ms
3. Sistema de monitoreo de drift de datos
4. Re-entrenamiento trimestral con nuevos datos

**M√©tricas de √âxito**:
- Latencia < 100ms por predicci√≥n
- Disponibilidad > 99.9%
- MCC > 0.95 en producci√≥n

---

#### Escenario 4: Investigaci√≥n Cl√≠nica (An√°lisis de Biomarcadores)
**Modelo Recomendado**: **Chebyshev-KAN V4**

**Justificaci√≥n**:
- Coeficientes polinomiales interpretables
- AUC-ROC m√°s alto ‚Üí Mejor ordenamiento de riesgo
- Permite an√°lisis de contribuci√≥n por caracter√≠stica

**Protocolo de An√°lisis**:
1. Entrenar Chebyshev-KAN y extraer coeficientes aprendidos
2. Analizar qu√© t√©rminos polinomiales tienen mayor magnitud
3. Identificar caracter√≠sticas con mayor contribuci√≥n no-lineal
4. Publicar insights sobre relaciones morfol√≥gicas vs malignidad

**M√©tricas de √âxito**:
- Identificaci√≥n de top-5 caracter√≠sticas m√°s discriminativas
- Cuantificaci√≥n de no-linealidades (grados 2-3 de Chebyshev)
- Correlaci√≥n con literatura m√©dica existente

---

### Limitaciones del Estudio

1. **Dataset Peque√±o**: 569 muestras pueden no capturar toda la variabilidad cl√≠nica
   - Recomendaci√≥n: Validar en datasets externos (DDSM, MIAS)

2. **Divisi√≥n Fija**: Resultados dependen de divisi√≥n train/val/test espec√≠fica
   - Recomendaci√≥n: Realizar validaci√≥n cruzada 10-fold

3. **Caracter√≠sticas Predefinidas**: Usa features extra√≠das, no im√°genes directas
   - Recomendaci√≥n: Explorar KAN en arquitecturas CNN end-to-end

4. **Fourier-KAN Bajo Desempe√±o**: Puede indicar inadecuaci√≥n de bases peri√≥dicas
   - Recomendaci√≥n: Probar bases mixtas (Fourier + Chebyshev h√≠brido)

5. **Sin An√°lisis de Incertidumbre**: No se cuantifica confianza de predicciones
   - Recomendaci√≥n: Integrar Dropout Bayesiano o Ensembles

6. **Interpretabilidad Limitada**: Aunque KAN ofrece coeficientes, no se analizan en profundidad
   - Recomendaci√≥n: Aplicar SHAP/LIME y an√°lisis de sensibilidad

---

### Trabajo Futuro

#### 1. Ensemble H√≠brido: MLP + Chebyshev-KAN
**Objetivo**: Combinar estabilidad de MLP con discriminaci√≥n de Chebyshev

**Arquitectura Propuesta**:
```python
Ensemble = 0.5 √ó MLP(x) + 0.5 √ó ChebyshevKAN(x)
```

**Expectativa**: Sensitivity ‚â• 0.98, Specificity ‚â• 0.98

---

#### 2. An√°lisis de Interpretabilidad Profunda
**M√©todos**:
- Extraer coeficientes de Chebyshev por capa y caracter√≠stica
- Visualizar contribuci√≥n de t√©rminos polinomiales (lineal, cuadr√°tico, c√∫bico)
- Correlacionar con literatura m√©dica sobre morfolog√≠a tumoral

**Preguntas de Investigaci√≥n**:
- ¬øQu√© caracter√≠sticas tienen relaciones m√°s no-lineales?
- ¬øC√≥mo se comparan con conocimiento m√©dico previo?

---

#### 3. Transfer Learning desde Datasets Grandes
**Dataset Fuente**: ImageNet pre-entrenado ‚Üí Fine-tuning en im√°genes mamogr√°ficas

**Objetivo**: Mejorar generalizaci√≥n con conocimiento previo

---

#### 4. Quantificaci√≥n de Incertidumbre
**M√©todo**: Monte Carlo Dropout o Deep Ensembles

**Aplicaci√≥n**: Estimar confianza por predicci√≥n ‚Üí Priorizar casos con alta incertidumbre para revisi√≥n humana

---

#### 5. Optimizaci√≥n de Hiperpar√°metros Bayesiana
**Herramienta**: Optuna con TPE (Tree-structured Parzen Estimator)

**Espacio de B√∫squeda**:
- Learning rate: [1e-4, 1e-2]
- Dropout: [0.1, 0.5]
- Arquitectura: N√∫mero de capas, neuronas por capa
- Grado de Chebyshev: [2, 5]

**Objetivo**: Encontrar configuraci√≥n √≥ptima globalmente

---

#### 6. Validaci√≥n Externa
**Datasets Adicionales**:
- **DDSM** (Digital Database for Screening Mammography)
- **MIAS** (Mammographic Image Analysis Society)
- **INbreast** (Portuguese mammography database)

**Objetivo**: Evaluar generalizaci√≥n cross-dataset

---

#### 7. Explicabilidad con SHAP
**Implementaci√≥n**: Aplicar SHAP (SHapley Additive exPlanations) a Chebyshev-KAN

**Visualizaciones**:
- SHAP waterfall plots por predicci√≥n
- SHAP summary plots por dataset
- Dependence plots de caracter√≠sticas clave

**Impacto**: Confianza cl√≠nica mediante explicaciones locales

---

#### 8. KAN en Arquitecturas CNN End-to-End
**Propuesta**: Reemplazar capas fully-connected finales de ResNet con Chebyshev-KAN

**Objetivo**: Combinar extracci√≥n autom√°tica de caracter√≠sticas (CNN) con aproximaci√≥n funcional (KAN)

---

#### 9. An√°lisis de Robustez
**Pruebas**:
- Perturbaciones adversariales (FGSM, PGD)
- Ruido Gaussiano en caracter√≠sticas
- Data augmentation (rotaciones, escalados en im√°genes originales)

**Objetivo**: Evaluar estabilidad de modelos ante variaciones

---

#### 10. Despliegue Cl√≠nico Piloto
**Fase 1**: Prueba retrospectiva en hospital colaborador

**Fase 2**: Estudio prospectivo comparando con radi√≥logos

**M√©tricas**:
- Concordancia inter-observador (Modelo vs M√©dico)
- Tiempo de diagn√≥stico (con vs sin IA)
- Satisfacci√≥n del personal m√©dico

---

## Referencias

### Papers Fundamentales
1. Kolmogorov, A. N. (1957). On the representation of continuous functions of several variables by superposition of continuous functions of one variable and addition.
2. Liu, Z. et al. (2024). KAN: Kolmogorov-Arnold Networks. arXiv:2404.19756.

### Datasets
1. Wolberg, W.H., Street, W.N., and Mangasarian, O.L. (1995). Wisconsin Diagnostic Breast Cancer (WDBC). UCI Machine Learning Repository.

### T√©cnicas de Regularizaci√≥n
1. Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training.
2. Srivastava, N. et al. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting.

### Optimizaci√≥n
1. Loshchilov, I., & Hutter, F. (2019). Decoupled Weight Decay Regularization (AdamW).

---

**Fin de la Documentaci√≥n T√©cnica**
